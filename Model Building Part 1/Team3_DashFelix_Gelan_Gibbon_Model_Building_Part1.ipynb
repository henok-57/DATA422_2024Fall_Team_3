{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a711729-ccb2-4729-802c-e55228da1b4a",
   "metadata": {},
   "source": [
    "# 1. Select your project questions and describe what they are and the methods you will use to answer them at the beginning of your write up.\n",
    "    How can we draw conclusions and predict the relationship between the US Dollar (USD) and Euro (EUR) exchange rates using Trend Analysis, Regression Analysis, & Correlation Analysis?\n",
    "\n",
    "    By integrating these data exploration techniques, we aim to build a comprehensive model for predicting Leg 4 of the EUROUSD currency pair.\n",
    "\n",
    "    Trend Analysis: The purpose for trend analysis is that it helps to identify long term patterns or directions in the data over time. For predicting leg 4 of the EUROUSD currency pair, trend analysis is vital to capture the underlying movement in the data, which is driven by some or all the technical and fundamental variables, such as legs, slopes, EMA, and miscellaneous variables. Understanding these trends can produce more accurate predictions of leg 4. For technique, I will break the data into four sections, January-March, April-June, July-September, October-December. From there, I will plot the distribution of the predictive variable (leg4) and observe if the distribution changes throughout the seasons. Means, medians, and variance will also be measured and compared across the four divided sections. Regarding relevance, understanding the seasonal trend in EUROUSD is critical for predicting leg 4. If the data shows variation between seasons, it could significantly influence the outcome of the predictive model. Trends help set a baseline for predictions, especially when combined with other factors like slopes and miscellaneous variables.\n",
    "    \n",
    "    Regression Analysis: The purpose of regression analysis is to help quantify the relationship between leg 4 and the other variables. It enables identification of which variables have significant predictive prowess, whereby a model can be constructed to predict leg 4, based on these variables. For technique, we will use multiple linear regression models, incorporating all variables from the Legs, Slopes, and Miscellaneous datasets. R squared will be used to determine the most significant predictors, which will then be incorporated into the model, and the metrics for model performance will be the classic Mean Squared Error (MSE). Regarding relevance, the regression analysis is crucial for predicting leg 4 because it not only highlights the strength of relationships between the variables, but also provides an objective mathematical model that can be used to make predictions.\n",
    "    \n",
    "    Correlation Analysis: The purpose of correlation analysis is to identify the strength and the direction of our linear relationships between leg 4 and the other variables. Understanding these correlations is useful in understanding the data structure and in selecting the proper variables to be used as model features. The technique that will be used is Pearson’s correlation coefficients. A correlation will be calculated for all other variables in the dataset besides the variable being predicted (leg 4). From here, a heatmap of correlation matrix will be constructed to visualize the relationships. The variables with high positive or negative correlations will be deemed significant. Regarding relevance, identifying variables that are highly correlated with leg 4 and only using those will improve the accuracy of our predictions. Secondly, it will help to detect issues of multicollinearity among the predictors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d60fb-0851-4636-a0d0-782fd2f05e76",
   "metadata": {},
   "source": [
    "# 2. You will need to select your models (3 methods) from the following types of techniques: regression, machine learning, clustering, forecasting, PCA, factor analysis.\n",
    "    Regression + PCA (Method):\n",
    "    Purpose- We will use Regression to model the relationship between the dependent variables in leg 4 & the predictors such as legs, slopes, and EMA’s. Using Principal Component Analysis (PCA) will be important in reducing dimensionality. This will help in showing what factors influence and affect direct changes in leg 4. By integrating regression with PCA, we can effectively model and quantify the impact of each predictor on leg4 while managing multicollinearity among variables. PCA simplifies the regression process by reducing the dataset to a few principal components that capture the most essential variance\n",
    "    Application- By applying the PCA technique to our dataset, it will reveal which underlying factors are the most influential in predicting patterns. This also shows us which part of our dataset leads to the most variability which could be used to further analyze influential predicting patterns. Using R Squared shows the variance in the dependent variables. The closer R Squared is to 1, the stronger variation in Leg 4 displayed in our model. Using Mean Squared Error shows the averaged squared difference between predicted and actual values. This is important because it will show how accurately the model’s predictions are and how close they are to actual results. Lower MSE values means the closer in predictions the model makes, while higher MSE values means that model is less accurate in making predictions. \n",
    "\n",
    "\n",
    "    Machine Learning + PCA (Method):\n",
    "    Purpose- We will use Machine Learning to model and predict the 4th and 5th leg based on previous historical trends to formulate a pattern. Basically we use past data to find patterns to predict what will happen in the future. To further explain, the model itself will learn over time and after collecting data & improve accuracy. \n",
    "    Find relationships between x variables (Unservervised) don't have answer aiming to find relationships. X variables + outcome figure out the answer\n",
    "    Application- We will use PCA to retain variability amongst overall components of data. Then we will use these components to input into our model so it can learn. The goal is to first predict leg 4 then later predict leg 5 after feeding it data from previous legs that we already have data on. This model will continuously learn, create predictive patterns and reduce complexity. Using PCA reduced data will reduce complex & nonlinear data that would confuse our model and make it difficult to draw conclusions and find regression. Overall this will improve accuracy and precision within our model.\n",
    "\n",
    "    \n",
    "    Factor Analysis + PCA (Method):\n",
    "    Purpose- We will use Factor Analysis to model and find important factors we might not have seen amongst observed variables. Using PCA will reveal which factors, and underlying variables that explain variances within our dataset. This will help isolate which group of variables make a significant difference when getting closer to our target variables. By also identifying the key factors, we want to reduce the dimensionality of our dataset and focus on the variables that contribute most to the overall structure of the data. This way can streamline our analysis and improve the accuracy of our predictive models by minimizing the noise from less impactful variables. \n",
    "    Application- We use PCA to determine the number of factors that will be the best for factor analysis. Then, factor analysis will determine which variables are important and make a difference in further predictions in our model. Factors are underlying variables that are latent and can be used to explain the dataset. After using PCA to decide the number of factors, we basically do our detailed analysis with factor loadings to assess each variable's association with each factor. This step reveals which factors most influence our variables, such as leg components and price pullbacks. If some factors do not capture the variance in slopes, price, or EMA as expected, we may increase the factor count or adjust our model to capture these dimensions better.\n",
    "\n",
    "\n",
    "    Neural Network + PCA (Method):\n",
    "    Purpose- We will use Neural Network to model strong nonlinear relationships within our dataset. This can help us isolate and locate subtle trends we might not have noticed before, by making sure only the significant components are fed into our model. Using a neural network allows us to capture complex, nonlinear relationships in the dataset that linear methods may overlook. By integrating PCA beforehand, we can distill the dataset to its most influential components, which helps prevent overfitting and allows the neural network to focus on significant trends and patterns relevant to predicting leg4.\n",
    "    Application- We will apply PCA to reduce dimensionality within our dataset, then design a neural network that will find the non linear patterns already within our data set. Our neural network trained on reduced PCA data will be precise and accurate in predicting leg 4. We can test this using RSME and MSE to see how the results are. After reducing the dataset's dimensionality through PCA, we construct a neural network tailored to learn the complex, nonlinear patterns that influence leg4. With the reduced PCA components as inputs, the neural network can train more efficiently, improving model accuracy and generalization. Performance is evaluated through metrics such as Root Mean Squared Error (RMSE) and Mean Squared Error (MSE), which allow us to assess prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157332d-7e49-433f-ba60-9a251d93155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST MODEL: FACTOR ANALYSIS + PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas import read_csv, Series, DataFrame\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd10e05-173d-46d2-8064-b3b9b4457704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_legs = pd.read_excel('Legs.xlsx')\n",
    "df_misc = pd.read_excel('Misc.xlsx')\n",
    "df_slopes = pd.read_excel('Slopes.xlsx')\n",
    "df_full = pd.read_excel('AllData(1).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccc837-68d7-4d50-9164-779c47e47484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make at least 2 models with at least 3 methods: regression, machine learning, clustering, forecasting, PCA, factor analysis\n",
    "# regression, clustering\n",
    "\n",
    "# factor analysis, first finding the number of factorss using PCA\n",
    "scaler = StandardScaler(with_std=True, with_mean=True)\n",
    "df = df_full.copy()\n",
    "df.columns = ['slope1', 'slope2', 'slope3', 'slope4', 'leg1', 'leg2', 'leg3', 'leg4', 'leg5', 'Price', 'EMA', 'Structure', 'Wick', 'Body']\n",
    "df.drop(columns = ['Structure'], inplace = True)\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "pca_df = PCA()\n",
    "pca_df.fit(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c964ed-6b87-47c1-aec6-d51dc7e7af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ticks = np.arange(pca_df.n_components_)+1\n",
    "ax = axes[0]\n",
    "ax.plot(ticks,\n",
    "        pca_df.explained_variance_ratio_,\n",
    "        marker='o')\n",
    "ax.set_xlabel('Principal Component');\n",
    "ax.set_ylabel('Proportion of Variance Explained')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xticks(ticks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87deaa60-6bca-45b7-83f4-98a388fb841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axes[1]\n",
    "ax.plot(ticks,\n",
    "        pca_df.explained_variance_ratio_.cumsum(),\n",
    "        marker='o')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Cumulative Proportion of Variance Explained')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xticks(ticks)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025481e-7719-4a57-9c77-5198edd85177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now doing the factor analysis\n",
    "# Factor 1 has high postitive loadings for body, leg4, leg3, and leg2 meaning that it is able to describe those well\n",
    "# Factor 3 is able to describe wick well\n",
    "# Factor 4 describes leg1 well\n",
    "labels = [\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\", \"Factor 6\", \"Factor 7\", \"Factor 8\", \"Factor 9\"]\n",
    "factors = 9\n",
    "fas = [\n",
    "    (\"FA unrotated\", FactorAnalysis(n_components = factors)),\n",
    "    (\"FA varimax\", FactorAnalysis(n_components = factors, rotation=\"varimax\")),\n",
    "]  \n",
    "fig, axes = plt.subplots(ncols=len(fas), figsize=(16, 11))\n",
    "\n",
    "for ax, (title, fa) in zip(axes, fas):\n",
    "    fa = fa.fit(scaled_df)\n",
    "    factor_matrix = fa.components_.T\n",
    "    im = ax.imshow(factor_matrix, cmap=\"RdBu_r\", vmax=1, vmin=-1)\n",
    "    for (i,j), z in np.ndenumerate(factor_matrix):\n",
    "        ax.text(j, i, str(z.round(2)), ha=\"center\", va=\"center\")\n",
    "    ax.set_yticks(np.arange(len(df.columns)))\n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_yticklabels(df.columns)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([0,1,2,3,4,5,6,7,8])\n",
    "    ax.xaxis.set_major_formatter(ticker.FixedFormatter((labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "cb = fig.colorbar(im, ax=axes, location='right', label=\"loadings\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717c3a1-61b7-46e8-a170-6de094370f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness\n",
    "# explains the variability that cannot be explained by a linear combo of the factors, so if it's large that means\n",
    "# that the factors don't account for it's variance that well\n",
    "fa = FactorAnalysis(n_components = 9, rotation=\"varimax\")\n",
    "fa.fit(scaled_df)\n",
    "uniqueness = Series(fa.noise_variance_, index=df.columns)\n",
    "uniqueness.plot(\n",
    "    kind=\"bar\",\n",
    "    ylabel=\"Uniqueness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef686ce8-dbb7-4532-9c21-cb5e8b91830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communality\n",
    "# Tells you about the fraction of the variable's total variance is explained by the factors\n",
    "# It is pretty high for leg2, leg3, wick, and body but very low for price\n",
    "communality = Series(np.square(fa.components_.T).sum(axis=1), index=df.columns)\n",
    "communality.plot(\n",
    "    kind=\"bar\",\n",
    "    ylabel=\"Communality\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08de5a0-fbb0-4442-b005-505d263f950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND MODEL: NEURAL NETWORK + PCA IF WE CANNOT UPDATE THE DATA\n",
    "\n",
    "# import openpyxl\n",
    "# print(openpyxl.__version__)  # Should output 3.1.5\n",
    "\n",
    "# # Instantiate the model, criterion, and optimizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "products_b = [] \n",
    "maxer_b = 100\n",
    "count_b = 0\n",
    "epoch_b = 0      \n",
    "products_ab = []  \n",
    "maxer_ab = 100\n",
    "count_ab = 0\n",
    "epoch_ab = 0\n",
    "products_abab = []  \n",
    "maxer_abab = 100\n",
    "count_abab = 0\n",
    "epoch_abab = 0\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics for plotting\n",
    "model1 = []\n",
    "model2 = []\n",
    "model3 = []\n",
    "model4 = []\n",
    "model5 = []\n",
    "model6 = []\n",
    "model7 = []\n",
    "model8 = []\n",
    "model9 = []\n",
    "model10 = []\n",
    "model11 = []\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = [] \n",
    "combined_losses = []   \n",
    "\n",
    "epoch_combined_min = 100\n",
    "epoch_adjusted_min = 100\n",
    "epoch_b_min = 100\n",
    "\n",
    "# Initialize minimum validation loss for comparison\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "# Define the Head class\n",
    "class Head(nn.Module):  # Defines a custom PyTorch module for a single attention head\n",
    "    def __init__(self, dim_model, head_size, dropout):  # Constructor to initialize the module\n",
    "        super().__init__()  # Initializes the parent class nn.Module\n",
    "        self.key = nn.Linear(dim_model, head_size, bias=False)  # Linear layer to project inputs into key space\n",
    "        self.query = nn.Linear(dim_model, head_size, bias=False)  # Linear layer to project inputs into query space\n",
    "        self.value = nn.Linear(dim_model, head_size, bias=False)  # Linear layer to project inputs into value space\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer to prevent overfitting by randomly setting some weights to zero\n",
    "\n",
    "    def forward(self, x):  # Forward pass function to process input tensor `x`\n",
    "        k = self.key(x)  # Computes the key projections from input `x`\n",
    "        q = self.query(x)  # Computes the query projections from input `x`\n",
    "        v = self.value(x)  # Computes the value projections from input `x`\n",
    "        weights = torch.matmul(q, k.transpose(-2, -1)) / (k.size(-1) ** 0.5)  # Calculates scaled dot-product attention weights\n",
    "        weights = F.softmax(weights, dim=-1)  # Applies softmax to normalize the attention weights along the last dimension\n",
    "        weights = self.dropout(weights)  # Applies dropout to the attention weights\n",
    "        out = torch.matmul(weights, v)  # Computes the weighted sum of value vectors\n",
    "        return out  # Returns the output of the attention head\n",
    "    \n",
    "# 1. The input x is a matrix of shape (batch_size, sequence_length, dim_model).\n",
    "# 2. Pass x through the key linear layer, resulting in matrix k of shape (batch_size, sequence_length, head_size)\n",
    "# 3. Pass x through the query linear layer, resulting in matrix q of shape (batch_size, sequence_length, head_size)\n",
    "# 4. Pass x through the value linear layer, resulting in matrix v of shape (batch_size, sequence_length, head_size)\n",
    "# 5. Next, we calculate the attention weights by performing a scaled dot product of q and the transpose of k. \n",
    "#    The result is normalized by dividing by the square root of the key dimension (head_size). This gives a matrix \n",
    "#    of shape (batch_size, sequence_length, sequence_length) representing how much each token attends to every \n",
    "#    other token in the sequence.\n",
    "# 6. The attention weights are passed through a softmax function along the last dimension (sequence length), \n",
    "#    normalizing them to sum to 1 for each query\n",
    "# 7. To prevent overfitting, dropout is applied: some weights are randomly set to zero during training.\n",
    "# 8. Finally, we perform a weighted sum of the value vectors using the attention weights. \n",
    "#    This produces the output matrix of shape (batch_size, sequence_length, head_size)\n",
    "\n",
    "\n",
    "# Define the MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):  # Defines a multi-head attention module\n",
    "    def __init__(self, num_heads, dim_model, head_size, dropout):  # Constructor to initialize the module\n",
    "        super().__init__()  # Calls the constructor of nn.Module\n",
    "        self.heads = nn.ModuleList([Head(dim_model, head_size, dropout) for _ in range(num_heads)])  # Creates a list of attention heads\n",
    "        self.linear = nn.Linear(num_heads * head_size, dim_model)  # Linear layer to combine the outputs of all heads\n",
    "        self.dropout = nn.Dropout(dropout)  # Adds a dropout layer for regularization\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # Concatenates the outputs of all heads along the last dimension\n",
    "        out = self.dropout(self.linear(out))  # Applies a linear transformation and dropout\n",
    "        return out  # Returns the final output\n",
    "\n",
    "# 1. The input x is a matrix of shape (batch_size, sequence_length, dim_model)\n",
    "# 2. The MultiHeadAttention module initializes multiple Head instances (num_heads in total). \n",
    "#    Each Head operates independently on the same input x\n",
    "# 3. In forward pass, x is passed through each attention head. Each head computes its own set of attention outputs\n",
    "# 4. The outputs from all heads are concatenated along the last dimension, resulting in a matrix of shape \n",
    "#    (batch_size, sequence_length, num_heads * head_size)\n",
    "# 5. The concatenated output is passed through a linear layer, projecting it back to the original dim_model dimensionality\n",
    "# 6. To prevent overfitting, dropout is applied to the output of the linear transformation\n",
    "# 7. The final output has the shape (batch_size, sequence_length, dim_model), which can be fed into the next layer of the model\n",
    "\n",
    "\n",
    "# Define the FeedForward class\n",
    "class FeedForward(nn.Module):  # Defines a feedforward neural network module\n",
    "    def __init__(self, dim_model, dropout):  # Constructor to initialize the module\n",
    "        super().__init__()  # Calls the constructor of nn.Module\n",
    "        self.net = nn.Sequential(  # Defines a sequential feedforward network\n",
    "            nn.Linear(dim_model, 4 * dim_model),  # First linear layer projects input to 4 times its dimensionality\n",
    "            nn.ReLU(),  # Applies ReLU activation function to introduce non-linearity\n",
    "            nn.Linear(4 * dim_model, dim_model),  # Second linear layer projects back to original dimension\n",
    "            nn.Dropout(dropout),  # Adds a dropout layer for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        return self.net(x)  # Passes x through the sequential feedforward network\n",
    "\n",
    "# 1. The input x is a matrix of shape (batch_size, sequence_length, dim_model)\n",
    "# 2. The input x is passed through the first nn.Linear layer, which projects the input from dim_model to \n",
    "#    4 * dim_model dimensions; Resulting shape: (batch_size, sequence_length, 4 * dim_model)\n",
    "# 3. The ReLU activation function is applied element-wise to introduce non-linearity and learn more complex patterns\n",
    "# 4. The output from the ReLU activation is passed through the second nn.Linear layer, which projects the data back \n",
    "#    from 4 * dim_model to original dim_model dimensions; Resulting shape: (batch_size, sequence_length, dim_model)\n",
    "# 5. Dropout is applied to the output of the second linear layer to help with overfitting\n",
    "# 6. The output is returned with the same shape as the input: (batch_size, sequence_length, dim_model)\n",
    "\n",
    "# Define the Block class\n",
    "class Block(nn.Module):  # Defines a transformer encoder block module\n",
    "    def __init__(self, dim_model, num_heads, head_size, dropout):  # Constructor to initialize the block\n",
    "        super().__init__()  # Calls the constructor of nn.Module\n",
    "        self.attention = MultiHeadAttention(num_heads, dim_model, head_size, dropout)  # Multi-head self-attention mechanism\n",
    "        self.ffwd = FeedForward(dim_model, dropout)  # Feedforward network\n",
    "        self.ln1 = nn.LayerNorm(dim_model)  # Layer normalization before self-attention\n",
    "        self.ln2 = nn.LayerNorm(dim_model)  # Layer normalization before feedforward\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        x = x + self.attention(self.ln1(x))  # Apply ln1, self-attention, and residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))  # Apply ln2, feedforward, and residual connection\n",
    "        return x  # Return the processed output\n",
    "\n",
    "# 1. The input x (shape: (batch_size, sequence_length, dim_model)) is first normalized using self.ln1\n",
    "#    Normalization ensures each token in the sequence has zero mean and unit variance along the dim_model dimension\n",
    "# 2. The normalized output from self.ln1(x) is passed into self.attention\n",
    "#    Multi-head attention computes self-attention over the sequence, capturing relationships between tokens in different positions\n",
    "# 3. The result of the attention mechanism is added back to the original input x\n",
    "#    This helps preserve the original information and mitigates the risk of vanishing gradients\n",
    "# 4. The updated x from the previous step is normalized again using self.ln2\n",
    "# 5. The normalized output from self.ln2(x) is passed into the feedforward network\n",
    "#    The feedforward network consists of two linear layers with a ReLU activation and dropout in between, applied independently to each token\n",
    "# 6. The output of the feedforward network is added back to the input of this step (the result from self.ln2(x)\n",
    "# 7. The final result x (shape: (batch_size, sequence_length, dim_model)) is returned\n",
    "#    This output contains refined token representations, capturing both contextual dependencies (via attention) and feature transformation (via feedforward)\n",
    "\n",
    "\n",
    "\n",
    "# Define the TransformerModel class\n",
    "class TransformerModel(nn.Module):  # Defines the Transformer model\n",
    "    def __init__(self, num_features, num_classes, dim_model, num_heads, num_layers, dropout):  # Constructor\n",
    "        super(TransformerModel, self).__init__()  # Calls the constructor of nn.Module\n",
    "        self.embedding = nn.Linear(num_features, dim_model)  # Embedding layer to project input features to dim_model\n",
    "        self.blocks = nn.Sequential(*[  # Sequential container for stacking multiple Block layers\n",
    "            Block(dim_model, num_heads, dim_model // num_heads, dropout)  # Each Block is a transformer encoder block\n",
    "            for _ in range(num_layers)  # Repeat for the specified number of layers\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(dim_model, num_classes)  # Fully connected layer to map dim_model to num_classes\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        x = self.embedding(x) * 0.1  # Project input features to dim_model and scale down\n",
    "        x = self.blocks(x) * 0.1  # Pass input through stacked transformer blocks and scale down\n",
    "        return self.fc_out(x)  # Pass the final output through a fully connected layer\n",
    "\n",
    "# 1. Input Shape: (batch_size, sequence_length, num_features)\n",
    "# 2. Embedding Layer (self.embedding): A nn.Linear layer that transforms each token's feature vector from \n",
    "#    num_features to dim_model. This is necessary because transformers operate in a space defined by dim_model\n",
    "# 3. Transformer Blocks (self.blocks): Sequentially applies multiple Block modules to refine token representations\n",
    "#    Multi-Head Attention: Captures relationships between tokens in the sequence.\n",
    "#    FeedForward Network: Adds non-linearity and transforms features.\n",
    "#    Residual Connections: Helps preserve original input and stabilize gradients.\n",
    "#    Layer Normalization: Normalizes features to improve training stability.\n",
    "# 4. Fully Connected Layer (self.fc_out): Maps the output from dim_model (transformer dimension) to num_classes \n",
    "#    (number of output categories per token).\n",
    "# 5. Return Output: The final output contains class scores for each token in the sequence. These scores can be used\n",
    "#    for classification tasks such as token classification, sequence labeling, etc\n",
    "\n",
    "\n",
    "# Load the data\n",
    "file_path = r\"C:\\Users\\ivonn\\Downloads\\AllData2.xlsx\"  # you might need to update file path prof\n",
    "data = pd.read_excel(file_path, header=None)\n",
    "X_train = data.iloc[:550, :13].values\n",
    "M_train = data.iloc[:550, 13].values\n",
    "N_train = data.iloc[:550, 14].values\n",
    "\n",
    "X_val = data.iloc[550:1100, :13].values\n",
    "M_val = data.iloc[550:1100, 13].values\n",
    "N_val = data.iloc[550:1100, 14].values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Creating a list of strategies, which is essentially a range of values that leg4 and leg5 can be.\n",
    "# The objective of this model is to predict leg4 and leg5, so I made a tuple called strategies which involves the\n",
    "# upper and lower bound of leg4 and the binary yes/no of if leg5 is at least some value, so 3 variables in the tuple\n",
    "strategies = []\n",
    "for entry in range(100,120):\n",
    "    for sl in range(150,170):\n",
    "        strategies.append((69, entry, sl)) \n",
    "\n",
    "# Prepare the targets\n",
    "Y_train = [[0 for _ in range(len(strategies))] for _ in range(len(X_train))]\n",
    "Y_val = [[0 for _ in range(len(strategies))] for _ in range(len(X_val))]\n",
    "\n",
    "\n",
    "# Populate Y_train and Y_val based my logic\n",
    "# This is how I score the model. It's nice because the higher the validation_profit, which you will see later, the\n",
    "# better this model is at actually predicting the price of EUROUSD and if validation_profit > 0, then the model \n",
    "# actually makes money\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(strategies)):\n",
    "        if M_train[i] > strategies[j][1] and M_train[i] < strategies[j][2] and N_train[i] > (100 - strategies[j][0]):\n",
    "            number = ((strategies[j][1]- strategies[j][0])/(strategies[j][2]-strategies[j][1]))\n",
    "            Y_train[i][j] = number # if model predicts correctly, append the profit made \n",
    "        elif M_train[i] > strategies[j][1] and M_train[i] > strategies[j][2]:\n",
    "            Y_train[i][j] = -1 # if actual leg4 value is outside of predicted boundries, you lose 1.0 or 100% of risk\n",
    "        elif M_train[i] > strategies[j][1] and M_train[i] < strategies[j][2] and N_train[i] < (100 - strategies[j][0]):\n",
    "            Y_train[i][j] = -1 # if actual leg5 value is less than predicted minimum of leg5, you lose 100% of your risk\n",
    "        elif M_train[i] < strategies[j][1]:\n",
    "            Y_train[i][j] = 0  # if there is no trade that exists, append nothing, so 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    for j in range(len(strategies)):\n",
    "        if M_val[i] > strategies[j][1] and M_val[i] < strategies[j][2] and N_val[i] > (100 - strategies[j][0]):\n",
    "            number = ((strategies[j][1]- strategies[j][0])/(strategies[j][2]-strategies[j][1]))\n",
    "            Y_val[i][j] = number # if model predicts correctly, append the profit made\n",
    "        elif M_val[i] > strategies[j][1] and M_val[i] > strategies[j][2]:\n",
    "            Y_val[i][j] = -1 # if actual leg4 value is outside of predicted boundries, you lose 1.0 or 100% of risk\n",
    "        elif M_val[i] > strategies[j][1] and M_val[i] < strategies[j][2] and N_val[i] < (100 - strategies[j][0]):\n",
    "            Y_val[i][j] = -1 # if actual leg5 value is less than predicted minimum of leg5, you lose 100% of your risk\n",
    "        elif M_val[i] < strategies[j][1]:\n",
    "            Y_val[i][j] = 0  # if there is no trade that exists, append nothing, so 0\n",
    "\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# Function to train or retrain a model\n",
    "\n",
    "\n",
    "# Update to train_or_retrain function\n",
    "def train_or_retrain(model, epochs, retraining=False, model_index=None):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.MSELoss()  # Use built-in MSE loss\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99999)\n",
    "    val_profits = []  # Initialize variable to store validation profits\n",
    "    val_losses = []  # Initialize variable to store validation losses\n",
    "    epoch_train_losses = []  # Store training loss for each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs_train = model(X_train_tensor)\n",
    "        loss_train = criterion(outputs_train, Y_train_tensor)\n",
    "        epoch_train_losses.append(loss_train.item())  # Append training loss for current epoch\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:  # Perform validation every 10 epochs and on the last epoch\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs_val = model(X_val_tensor)\n",
    "                val_loss = criterion(outputs_val, Y_val_tensor)\n",
    "                _, predicted = torch.max(outputs_val, 1)\n",
    "                val_profit = Y_val_tensor[range(len(Y_val_tensor)), predicted].sum().item()\n",
    "\n",
    "                # Store profits in corresponding model list based on model_index\n",
    "                if model_index == 0:\n",
    "                    model1.append(val_profit)\n",
    "                elif model_index == 1:\n",
    "                    model2.append(val_profit)\n",
    "                elif model_index == 2:\n",
    "                    model3.append(val_profit)\n",
    "                elif model_index == 3:\n",
    "                    model4.append(val_profit)\n",
    "                elif model_index == 4:\n",
    "                    model5.append(val_profit)\n",
    "                elif model_index == 5:\n",
    "                    model6.append(val_profit)\n",
    "                elif model_index == 6:\n",
    "                    model7.append(val_profit)\n",
    "                elif model_index == 7:\n",
    "                    model8.append(val_profit)\n",
    "                elif model_index == 8:\n",
    "                    model9.append(val_profit)\n",
    "                elif model_index == 9:\n",
    "                    model10.append(val_profit)\n",
    "                elif model_index == 10:\n",
    "                    model11.append(val_profit)\n",
    "\n",
    "                val_profits.append(val_profit)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        # Calculate mean of all validation profits up to the current epoch\n",
    "        all_val_profits_mean = np.mean(val_profits)\n",
    "\n",
    "        # Print the current state\n",
    "        print(f\"{'Retraining' if retraining else 'Training'} model, Epoch {epoch}: Train Loss: {loss_train.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Profit1: {val_profit:.4f}, All Val Average: {all_val_profits_mean:.4f}\")\n",
    "\n",
    "        # Early stopping if last 20 validation profits are all positive\n",
    "        recent_val_profits = val_profits[-20:]\n",
    "        if len(recent_val_profits) == 20 and all(profit > 0 for profit in recent_val_profits):\n",
    "            print(\"Breaking: Last 20 validation profits were all positive.\")\n",
    "            return epoch_train_losses, val_profits[-1], val_losses[-1]\n",
    "\n",
    "    return epoch_train_losses, val_profits[-1], val_losses[-1]\n",
    "\n",
    "# Training Loop\n",
    "num_models = 11  # Number of models in the ensemble\n",
    "epochs = 500  # Number of epochs to train each model\n",
    "all_model_train_losses = []  # Collect all training losses from all models\n",
    "models = []  # Store trained models\n",
    "ensemble_val_profits1 = []  # Store final val profits for ensemble evaluation\n",
    "\n",
    "for i in range(num_models):\n",
    "    print(f\"Training model {i+1}/{num_models}\")\n",
    "    model = TransformerModel(num_features=13, num_classes=len(strategies), dim_model=512, num_heads=8, num_layers=4, dropout=0.2)\n",
    "    \n",
    "    # Train the model and collect losses\n",
    "    epoch_train_losses, last_val_profit, last_val_loss = train_or_retrain(model, epochs, model_index=i)\n",
    "    all_model_train_losses.append(epoch_train_losses)  # Collect epoch-wise training losses for analysis\n",
    "    models.append(model)  # Store the trained model\n",
    "    \n",
    "    print(f\"Model {i+1} added to ensemble with Last Val Profit1: {last_val_profit:.4f}, Last Val Loss: {last_val_loss:.4f}\")\n",
    "\n",
    "# Calculate and Print Required Averages\n",
    "average_loss_epoch_0 = np.mean([losses[0] for losses in all_model_train_losses])\n",
    "average_loss_epoch_190 = np.mean([losses[190] for losses in all_model_train_losses])\n",
    "average_loss_all_epochs = np.mean([np.mean(losses) for losses in all_model_train_losses])\n",
    "\n",
    "print(f\"Average Training Loss at Epoch 0: {average_loss_epoch_0:.4f}\")\n",
    "print(f\"Average Training Loss at Epoch 190: {average_loss_epoch_190:.4f}\")\n",
    "print(f\"Average Training Loss Across All Epochs: {average_loss_all_epochs:.4f}\")\n",
    "\n",
    "# Ensemble Performance Evaluation\n",
    "ensemble_predictions = torch.zeros_like(Y_val_tensor)  # Initialize a tensor with the same shape as Y_val_tensor to store ensemble predictions\n",
    "for model in models:  # Loop through each trained model in the ensemble\n",
    "    model.eval()  # Set the model to evaluation mode to disable dropout and batch normalization updates\n",
    "    with torch.no_grad():  # Disable gradient calculations to save memory and computation during inference\n",
    "        outputs_val = model(X_val_tensor)  # Generate predictions on the validation dataset\n",
    "        ensemble_predictions += outputs_val / len(models)  # Accumulate and average predictions across all models in the ensemble\n",
    "\n",
    "_, predicted = torch.max(ensemble_predictions, 1)  # Find the index of the maximum predicted value along the second dimension\n",
    "profit1 = Y_val_tensor[range(len(Y_val_tensor)), predicted].sum().item()  # Sum up the profits based on the predicted indices for the validation dataset\n",
    "ensemble_val_profits1.append(profit1)  # Store the computed validation profit for the ensemble in the list\n",
    "\n",
    "print(f\"Ensemble's Val Profit1: {ensemble_val_profits1[-1]:.4f}\")  # Print the last validation profit value for the ensemble\n",
    "\n",
    "\n",
    "for a, b in zip(training_losses, validation_losses):  # Iterate over pairs of training and validation losses\n",
    "    count_b += 1  # Increment the count for validation losses\n",
    "    product_b = b  # Assign the current validation loss to product_b\n",
    "    products_b.append(product_b)  # Append the current validation loss to the products_b list\n",
    "    if product_b < maxer_b:  # Check if the current validation loss is the lowest encountered so far\n",
    "        maxer_b = product_b  # Update maxer_b with the new lowest validation loss\n",
    "        epoch_b_min = count_b  # Record the epoch corresponding to the lowest validation loss\n",
    "    \n",
    "    count_ab += 1  # Increment the count for combined losses\n",
    "    product_ab = a + b  # Calculate the sum of the current training and validation losses\n",
    "    products_ab.append(product_ab)  # Append the combined loss to the products_ab list\n",
    "    if product_ab < maxer_ab:  # Check if the current combined loss is the lowest encountered so far\n",
    "        maxer_ab = product_ab  # Update maxer_ab with the new lowest combined loss\n",
    "        epoch_combined_min = count_ab  # Record the epoch corresponding to the lowest combined loss\n",
    "    \n",
    "    count_abab += 1  # Increment the count for adjusted losses\n",
    "    product_abab = (a + b) * (a * b)  # Calculate the adjusted loss as the product of the sum and product of the current training and validation losses\n",
    "    products_abab.append(product_abab)  # Append the adjusted loss to the products_abab list\n",
    "    if product_abab < maxer_abab:  # Check if the current adjusted loss is the lowest encountered so far\n",
    "        maxer_abab = product_abab  # Update maxer_abab with the new lowest adjusted loss\n",
    "        epoch_adjusted_min = count_abab  # Record the epoch corresponding to the lowest adjusted loss\n",
    "\n",
    "        \n",
    "# Plotting all metrics and vertical lines at epochs of interest\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ensemble_val_profits1, label='ensemble models')\n",
    "plt.plot(model1, label='model1')\n",
    "plt.plot(model2, label='model2')\n",
    "plt.plot(model3, label='model3')\n",
    "plt.plot(model4, label='model4')\n",
    "plt.plot(model5, label='model5')\n",
    "plt.plot(model6, label='model6')\n",
    "plt.plot(model7, label='model7')\n",
    "plt.plot(model8, label='model8')\n",
    "plt.plot(model9, label='model9')\n",
    "plt.plot(model10, label='model10')\n",
    "plt.plot(model11, label='model11')\n",
    "\n",
    "plt.plot(training_losses, label='Training Loss', linestyle='--')\n",
    "plt.plot(validation_losses, label='Validation Loss', linestyle='--')\n",
    "plt.plot(combined_losses, label='Combined Loss', linestyle='--')\n",
    "\n",
    "\n",
    "plt.title('Validation Profits and Losses Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Profit')\n",
    "plt.legend()\n",
    "plt.show() # add running average of val profit1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c768592-f666-4b34-a8fd-d6232eac9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND MODEL IF WE CAN UPDATE THE DATA\n",
    "\n",
    "\n",
    "# import openpyxl\n",
    "# print(openpyxl.__version__)  # Should output 3.1.5\n",
    "\n",
    "\n",
    "\n",
    "# # Instantiate the model, criterion, and optimizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "products_b = [] \n",
    "maxer_b = 100\n",
    "count_b = 0\n",
    "epoch_b = 0      \n",
    "products_ab = []  \n",
    "maxer_ab = 100\n",
    "count_ab = 0\n",
    "epoch_ab = 0\n",
    "products_abab = []  \n",
    "maxer_abab = 100\n",
    "count_abab = 0\n",
    "epoch_abab = 0\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics for plotting\n",
    "model1 = []\n",
    "model2 = []\n",
    "model3 = []\n",
    "model4 = []\n",
    "model5 = []\n",
    "model6 = []\n",
    "model7 = []\n",
    "model8 = []\n",
    "model9 = []\n",
    "model10 = []\n",
    "model11 = []\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = [] \n",
    "combined_losses = []   \n",
    "\n",
    "epoch_combined_min = 100\n",
    "epoch_adjusted_min = 100\n",
    "epoch_b_min = 100\n",
    "\n",
    "# Initialize minimum validation loss for comparison\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "# Define the Head class\n",
    "class Head(nn.Module):  # Defines a custom PyTorch module for a single attention head\n",
    "    def __init__(self, dim_model, head_size, dropout):  # Constructor to initialize the module\n",
    "        super().__init__()  # Initializes the parent class nn.Module\n",
    "        self.key = nn.Linear(dim_model, head_size, bias=False)  # Linear layer to project inputs into key space\n",
    "        self.query = nn.Linear(dim_model, head_size, bias=False)  # Linear layer to project inputs into query space\n",
    "        self.value = nn.Linear(dim_model, head_size, bias=False)  # Linear layer to project inputs into value space\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer to prevent overfitting by randomly setting some weights to zero\n",
    "\n",
    "    def forward(self, x):  # Forward pass function to process input tensor `x`\n",
    "        k = self.key(x)  # Computes the key projections from input `x`\n",
    "        q = self.query(x)  # Computes the query projections from input `x`\n",
    "        v = self.value(x)  # Computes the value projections from input `x`\n",
    "        weights = torch.matmul(q, k.transpose(-2, -1)) / (k.size(-1) ** 0.5)  # Calculates scaled dot-product attention weights\n",
    "        weights = F.softmax(weights, dim=-1)  # Applies softmax to normalize the attention weights along the last dimension\n",
    "        weights = self.dropout(weights)  # Applies dropout to the attention weights\n",
    "        out = torch.matmul(weights, v)  # Computes the weighted sum of value vectors\n",
    "        return out  # Returns the output of the attention head\n",
    "    \n",
    "# 1. The input x is a matrix of shape (batch_size, sequence_length, dim_model).\n",
    "# 2. Pass x through the key linear layer, resulting in matrix k of shape (batch_size, sequence_length, head_size)\n",
    "# 3. Pass x through the query linear layer, resulting in matrix q of shape (batch_size, sequence_length, head_size)\n",
    "# 4. Pass x through the value linear layer, resulting in matrix v of shape (batch_size, sequence_length, head_size)\n",
    "# 5. Next, we calculate the attention weights by performing a scaled dot product of q and the transpose of k. \n",
    "#    The result is normalized by dividing by the square root of the key dimension (head_size). This gives a matrix \n",
    "#    of shape (batch_size, sequence_length, sequence_length) representing how much each token attends to every \n",
    "#    other token in the sequence.\n",
    "# 6. The attention weights are passed through a softmax function along the last dimension (sequence length), \n",
    "#    normalizing them to sum to 1 for each query\n",
    "# 7. To prevent overfitting, dropout is applied: some weights are randomly set to zero during training.\n",
    "# 8. Finally, we perform a weighted sum of the value vectors using the attention weights. \n",
    "#    This produces the output matrix of shape (batch_size, sequence_length, head_size)\n",
    "\n",
    "\n",
    "# Define the MultiHeadAttention class\n",
    "class MultiHeadAttention(nn.Module):  # Defines a multi-head attention module\n",
    "    def __init__(self, num_heads, dim_model, head_size, dropout):  # Constructor to initialize the module\n",
    "        super().__init__()  # Calls the constructor of nn.Module\n",
    "        self.heads = nn.ModuleList([Head(dim_model, head_size, dropout) for _ in range(num_heads)])  # Creates a list of attention heads\n",
    "        self.linear = nn.Linear(num_heads * head_size, dim_model)  # Linear layer to combine the outputs of all heads\n",
    "        self.dropout = nn.Dropout(dropout)  # Adds a dropout layer for regularization\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # Concatenates the outputs of all heads along the last dimension\n",
    "        out = self.dropout(self.linear(out))  # Applies a linear transformation and dropout\n",
    "        return out  # Returns the final output\n",
    "\n",
    "# 1. The input x is a matrix of shape (batch_size, sequence_length, dim_model)\n",
    "# 2. The MultiHeadAttention module initializes multiple Head instances (num_heads in total). \n",
    "#    Each Head operates independently on the same input x\n",
    "# 3. In forward pass, x is passed through each attention head. Each head computes its own set of attention outputs\n",
    "# 4. The outputs from all heads are concatenated along the last dimension, resulting in a matrix of shape \n",
    "#    (batch_size, sequence_length, num_heads * head_size)\n",
    "# 5. The concatenated output is passed through a linear layer, projecting it back to the original dim_model dimensionality\n",
    "# 6. To prevent overfitting, dropout is applied to the output of the linear transformation\n",
    "# 7. The final output has the shape (batch_size, sequence_length, dim_model), which can be fed into the next layer of the model\n",
    "\n",
    "\n",
    "# Define the FeedForward class\n",
    "class FeedForward(nn.Module):  # Defines a feedforward neural network module\n",
    "    def __init__(self, dim_model, dropout):  # Constructor to initialize the module\n",
    "        super().__init__()  # Calls the constructor of nn.Module\n",
    "        self.net = nn.Sequential(  # Defines a sequential feedforward network\n",
    "            nn.Linear(dim_model, 4 * dim_model),  # First linear layer projects input to 4 times its dimensionality\n",
    "            nn.ReLU(),  # Applies ReLU activation function to introduce non-linearity\n",
    "            nn.Linear(4 * dim_model, dim_model),  # Second linear layer projects back to original dimension\n",
    "            nn.Dropout(dropout),  # Adds a dropout layer for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        return self.net(x)  # Passes x through the sequential feedforward network\n",
    "\n",
    "# 1. The input x is a matrix of shape (batch_size, sequence_length, dim_model)\n",
    "# 2. The input x is passed through the first nn.Linear layer, which projects the input from dim_model to \n",
    "#    4 * dim_model dimensions; Resulting shape: (batch_size, sequence_length, 4 * dim_model)\n",
    "# 3. The ReLU activation function is applied element-wise to introduce non-linearity and learn more complex patterns\n",
    "# 4. The output from the ReLU activation is passed through the second nn.Linear layer, which projects the data back \n",
    "#    from 4 * dim_model to original dim_model dimensions; Resulting shape: (batch_size, sequence_length, dim_model)\n",
    "# 5. Dropout is applied to the output of the second linear layer to help with overfitting\n",
    "# 6. The output is returned with the same shape as the input: (batch_size, sequence_length, dim_model)\n",
    "\n",
    "# Define the Block class\n",
    "class Block(nn.Module):  # Defines a transformer encoder block module\n",
    "    def __init__(self, dim_model, num_heads, head_size, dropout):  # Constructor to initialize the block\n",
    "        super().__init__()  # Calls the constructor of nn.Module\n",
    "        self.attention = MultiHeadAttention(num_heads, dim_model, head_size, dropout)  # Multi-head self-attention mechanism\n",
    "        self.ffwd = FeedForward(dim_model, dropout)  # Feedforward network\n",
    "        self.ln1 = nn.LayerNorm(dim_model)  # Layer normalization before self-attention\n",
    "        self.ln2 = nn.LayerNorm(dim_model)  # Layer normalization before feedforward\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        x = x + self.attention(self.ln1(x))  # Apply ln1, self-attention, and residual connection\n",
    "        x = x + self.ffwd(self.ln2(x))  # Apply ln2, feedforward, and residual connection\n",
    "        return x  # Return the processed output\n",
    "\n",
    "# 1. The input x (shape: (batch_size, sequence_length, dim_model)) is first normalized using self.ln1\n",
    "#    Normalization ensures each token in the sequence has zero mean and unit variance along the dim_model dimension\n",
    "# 2. The normalized output from self.ln1(x) is passed into self.attention\n",
    "#    Multi-head attention computes self-attention over the sequence, capturing relationships between tokens in different positions\n",
    "# 3. The result of the attention mechanism is added back to the original input x\n",
    "#    This helps preserve the original information and mitigates the risk of vanishing gradients\n",
    "# 4. The updated x from the previous step is normalized again using self.ln2\n",
    "# 5. The normalized output from self.ln2(x) is passed into the feedforward network\n",
    "#    The feedforward network consists of two linear layers with a ReLU activation and dropout in between, applied independently to each token\n",
    "# 6. The output of the feedforward network is added back to the input of this step (the result from self.ln2(x)\n",
    "# 7. The final result x (shape: (batch_size, sequence_length, dim_model)) is returned\n",
    "#    This output contains refined token representations, capturing both contextual dependencies (via attention) and feature transformation (via feedforward)\n",
    "\n",
    "\n",
    "\n",
    "# Define the TransformerModel class\n",
    "class TransformerModel(nn.Module):  # Defines the Transformer model\n",
    "    def __init__(self, num_features, num_classes, dim_model, num_heads, num_layers, dropout):  # Constructor\n",
    "        super(TransformerModel, self).__init__()  # Calls the constructor of nn.Module\n",
    "        self.embedding = nn.Linear(num_features, dim_model)  # Embedding layer to project input features to dim_model\n",
    "        self.blocks = nn.Sequential(*[  # Sequential container for stacking multiple Block layers\n",
    "            Block(dim_model, num_heads, dim_model // num_heads, dropout)  # Each Block is a transformer encoder block\n",
    "            for _ in range(num_layers)  # Repeat for the specified number of layers\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(dim_model, num_classes)  # Fully connected layer to map dim_model to num_classes\n",
    "\n",
    "    def forward(self, x):  # Forward method to process input x\n",
    "        x = self.embedding(x) * 0.1  # Project input features to dim_model and scale down\n",
    "        x = self.blocks(x) * 0.1  # Pass input through stacked transformer blocks and scale down\n",
    "        return self.fc_out(x)  # Pass the final output through a fully connected layer\n",
    "\n",
    "# 1. Input Shape: (batch_size, sequence_length, num_features)\n",
    "# 2. Embedding Layer (self.embedding): A nn.Linear layer that transforms each token's feature vector from \n",
    "#    num_features to dim_model. This is necessary because transformers operate in a space defined by dim_model\n",
    "# 3. Transformer Blocks (self.blocks): Sequentially applies multiple Block modules to refine token representations\n",
    "#    Multi-Head Attention: Captures relationships between tokens in the sequence.\n",
    "#    FeedForward Network: Adds non-linearity and transforms features.\n",
    "#    Residual Connections: Helps preserve original input and stabilize gradients.\n",
    "#    Layer Normalization: Normalizes features to improve training stability.\n",
    "# 4. Fully Connected Layer (self.fc_out): Maps the output from dim_model (transformer dimension) to num_classes \n",
    "#    (number of output categories per token).\n",
    "# 5. Return Output: The final output contains class scores for each token in the sequence. These scores can be used\n",
    "#    for classification tasks such as token classification, sequence labeling, etc\n",
    "\n",
    "\n",
    "# Load the data\n",
    "file_path = r\"C:\\Users\\ivonn\\Downloads\\MoreData.xlsx\"  # you might need to update file path prof\n",
    "data = pd.read_excel(file_path, header=None)\n",
    "X_train = data.iloc[:1400, :13].values\n",
    "M_train = data.iloc[:1400, 13].values\n",
    "N_train = data.iloc[:1400, 14].values\n",
    "\n",
    "X_val = data.iloc[1400:2100, :13].values\n",
    "M_val = data.iloc[1400:2100, 13].values\n",
    "N_val = data.iloc[1400:2100, 14].values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Creating a list of strategies, which is essentially a range of values that leg4 and leg5 can be.\n",
    "# The objective of this model is to predict leg4 and leg5, so I made a tuple called strategies which involves the\n",
    "# upper and lower bound of leg4 and the binary yes/no of if leg5 is at least some value, so 3 variables in the tuple\n",
    "strategies = []\n",
    "for entry in range(100,120):\n",
    "    for sl in range(150,170):\n",
    "        strategies.append((69, entry, sl)) \n",
    "\n",
    "# Prepare the targets\n",
    "Y_train = [[0 for _ in range(len(strategies))] for _ in range(len(X_train))]\n",
    "Y_val = [[0 for _ in range(len(strategies))] for _ in range(len(X_val))]\n",
    "\n",
    "\n",
    "# Populate Y_train and Y_val based my logic\n",
    "# This is how I score the model. It's nice because the higher the validation_profit, which you will see later, the\n",
    "# better this model is at actually predicting the price of EUROUSD and if validation_profit > 0, then the model \n",
    "# actually makes money\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(strategies)):\n",
    "        if M_train[i] > strategies[j][1] and M_train[i] < strategies[j][2] and N_train[i] > (100 - strategies[j][0]):\n",
    "            number = ((strategies[j][1]- strategies[j][0])/(strategies[j][2]-strategies[j][1]))\n",
    "            Y_train[i][j] = number # if model predicts correctly, append the profit made \n",
    "        elif M_train[i] > strategies[j][1] and M_train[i] > strategies[j][2]:\n",
    "            Y_train[i][j] = -1 # if actual leg4 value is outside of predicted boundries, you lose 1.0 or 100% of risk\n",
    "        elif M_train[i] > strategies[j][1] and M_train[i] < strategies[j][2] and N_train[i] < (100 - strategies[j][0]):\n",
    "            Y_train[i][j] = -1 # if actual leg5 value is less than predicted minimum of leg5, you lose 100% of your risk\n",
    "        elif M_train[i] < strategies[j][1]:\n",
    "            Y_train[i][j] = 0  # if there is no trade that exists, append nothing, so 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    for j in range(len(strategies)):\n",
    "        if M_val[i] > strategies[j][1] and M_val[i] < strategies[j][2] and N_val[i] > (100 - strategies[j][0]):\n",
    "            number = ((strategies[j][1]- strategies[j][0])/(strategies[j][2]-strategies[j][1]))\n",
    "            Y_val[i][j] = number # if model predicts correctly, append the profit made\n",
    "        elif M_val[i] > strategies[j][1] and M_val[i] > strategies[j][2]:\n",
    "            Y_val[i][j] = -1 # if actual leg4 value is outside of predicted boundries, you lose 1.0 or 100% of risk\n",
    "        elif M_val[i] > strategies[j][1] and M_val[i] < strategies[j][2] and N_val[i] < (100 - strategies[j][0]):\n",
    "            Y_val[i][j] = -1 # if actual leg5 value is less than predicted minimum of leg5, you lose 100% of your risk\n",
    "        elif M_val[i] < strategies[j][1]:\n",
    "            Y_val[i][j] = 0  # if there is no trade that exists, append nothing, so 0\n",
    "\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# Function to train or retrain a model\n",
    "\n",
    "\n",
    "# Update to train_or_retrain function\n",
    "def train_or_retrain(model, epochs, retraining=False, model_index=None):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.MSELoss()  # Use built-in MSE loss\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99999)\n",
    "    val_profits = []  # Initialize variable to store validation profits\n",
    "    val_losses = []  # Initialize variable to store validation losses\n",
    "    epoch_train_losses = []  # Store training loss for each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs_train = model(X_train_tensor)\n",
    "        loss_train = criterion(outputs_train, Y_train_tensor)\n",
    "        epoch_train_losses.append(loss_train.item())  # Append training loss for current epoch\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:  # Perform validation every 10 epochs and on the last epoch\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs_val = model(X_val_tensor)\n",
    "                val_loss = criterion(outputs_val, Y_val_tensor)\n",
    "                _, predicted = torch.max(outputs_val, 1)\n",
    "                val_profit = Y_val_tensor[range(len(Y_val_tensor)), predicted].sum().item()\n",
    "\n",
    "                # Store profits in corresponding model list based on model_index\n",
    "                if model_index == 0:\n",
    "                    model1.append(val_profit)\n",
    "                elif model_index == 1:\n",
    "                    model2.append(val_profit)\n",
    "                elif model_index == 2:\n",
    "                    model3.append(val_profit)\n",
    "                elif model_index == 3:\n",
    "                    model4.append(val_profit)\n",
    "                elif model_index == 4:\n",
    "                    model5.append(val_profit)\n",
    "                elif model_index == 5:\n",
    "                    model6.append(val_profit)\n",
    "                elif model_index == 6:\n",
    "                    model7.append(val_profit)\n",
    "                elif model_index == 7:\n",
    "                    model8.append(val_profit)\n",
    "                elif model_index == 8:\n",
    "                    model9.append(val_profit)\n",
    "                elif model_index == 9:\n",
    "                    model10.append(val_profit)\n",
    "                elif model_index == 10:\n",
    "                    model11.append(val_profit)\n",
    "\n",
    "                val_profits.append(val_profit)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        # Calculate mean of all validation profits up to the current epoch\n",
    "        all_val_profits_mean = np.mean(val_profits)\n",
    "\n",
    "        # Print the current state\n",
    "        print(f\"{'Retraining' if retraining else 'Training'} model, Epoch {epoch}: Train Loss: {loss_train.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Profit1: {val_profit:.4f}, All Val Average: {all_val_profits_mean:.4f}\")\n",
    "\n",
    "        # Early stopping if last 20 validation profits are all positive\n",
    "        recent_val_profits = val_profits[-20:]\n",
    "        if len(recent_val_profits) == 20 and all(profit > 0 for profit in recent_val_profits):\n",
    "            print(\"Breaking: Last 20 validation profits were all positive.\")\n",
    "            return epoch_train_losses, val_profits[-1], val_losses[-1]\n",
    "\n",
    "    return epoch_train_losses, val_profits[-1], val_losses[-1]\n",
    "\n",
    "# Training Loop\n",
    "num_models = 11  # Number of models in the ensemble\n",
    "epochs = 500  # Number of epochs to train each model\n",
    "all_model_train_losses = []  # Collect all training losses from all models\n",
    "models = []  # Store trained models\n",
    "ensemble_val_profits1 = []  # Store final val profits for ensemble evaluation\n",
    "\n",
    "for i in range(num_models):\n",
    "    print(f\"Training model {i+1}/{num_models}\")\n",
    "    model = TransformerModel(num_features=13, num_classes=len(strategies), dim_model=512, num_heads=8, num_layers=4, dropout=0.2)\n",
    "    \n",
    "    # Train the model and collect losses\n",
    "    epoch_train_losses, last_val_profit, last_val_loss = train_or_retrain(model, epochs, model_index=i)\n",
    "    all_model_train_losses.append(epoch_train_losses)  # Collect epoch-wise training losses for analysis\n",
    "    models.append(model)  # Store the trained model\n",
    "    \n",
    "    print(f\"Model {i+1} added to ensemble with Last Val Profit1: {last_val_profit:.4f}, Last Val Loss: {last_val_loss:.4f}\")\n",
    "\n",
    "# Calculate and Print Required Averages\n",
    "average_loss_epoch_0 = np.mean([losses[0] for losses in all_model_train_losses])\n",
    "average_loss_epoch_190 = np.mean([losses[190] for losses in all_model_train_losses])\n",
    "average_loss_all_epochs = np.mean([np.mean(losses) for losses in all_model_train_losses])\n",
    "\n",
    "print(f\"Average Training Loss at Epoch 0: {average_loss_epoch_0:.4f}\")\n",
    "print(f\"Average Training Loss at Epoch 190: {average_loss_epoch_190:.4f}\")\n",
    "print(f\"Average Training Loss Across All Epochs: {average_loss_all_epochs:.4f}\")\n",
    "\n",
    "# Ensemble Performance Evaluation\n",
    "ensemble_predictions = torch.zeros_like(Y_val_tensor)  # Initialize a tensor with the same shape as Y_val_tensor to store ensemble predictions\n",
    "for model in models:  # Loop through each trained model in the ensemble\n",
    "    model.eval()  # Set the model to evaluation mode to disable dropout and batch normalization updates\n",
    "    with torch.no_grad():  # Disable gradient calculations to save memory and computation during inference\n",
    "        outputs_val = model(X_val_tensor)  # Generate predictions on the validation dataset\n",
    "        ensemble_predictions += outputs_val / len(models)  # Accumulate and average predictions across all models in the ensemble\n",
    "\n",
    "_, predicted = torch.max(ensemble_predictions, 1)  # Find the index of the maximum predicted value along the second dimension\n",
    "profit1 = Y_val_tensor[range(len(Y_val_tensor)), predicted].sum().item()  # Sum up the profits based on the predicted indices for the validation dataset\n",
    "ensemble_val_profits1.append(profit1)  # Store the computed validation profit for the ensemble in the list\n",
    "\n",
    "print(f\"Ensemble's Val Profit1: {ensemble_val_profits1[-1]:.4f}\")  # Print the last validation profit value for the ensemble\n",
    "\n",
    "\n",
    "for a, b in zip(training_losses, validation_losses):  # Iterate over pairs of training and validation losses\n",
    "    count_b += 1  # Increment the count for validation losses\n",
    "    product_b = b  # Assign the current validation loss to product_b\n",
    "    products_b.append(product_b)  # Append the current validation loss to the products_b list\n",
    "    if product_b < maxer_b:  # Check if the current validation loss is the lowest encountered so far\n",
    "        maxer_b = product_b  # Update maxer_b with the new lowest validation loss\n",
    "        epoch_b_min = count_b  # Record the epoch corresponding to the lowest validation loss\n",
    "    \n",
    "    count_ab += 1  # Increment the count for combined losses\n",
    "    product_ab = a + b  # Calculate the sum of the current training and validation losses\n",
    "    products_ab.append(product_ab)  # Append the combined loss to the products_ab list\n",
    "    if product_ab < maxer_ab:  # Check if the current combined loss is the lowest encountered so far\n",
    "        maxer_ab = product_ab  # Update maxer_ab with the new lowest combined loss\n",
    "        epoch_combined_min = count_ab  # Record the epoch corresponding to the lowest combined loss\n",
    "    \n",
    "    count_abab += 1  # Increment the count for adjusted losses\n",
    "    product_abab = (a + b) * (a * b)  # Calculate the adjusted loss as the product of the sum and product of the current training and validation losses\n",
    "    products_abab.append(product_abab)  # Append the adjusted loss to the products_abab list\n",
    "    if product_abab < maxer_abab:  # Check if the current adjusted loss is the lowest encountered so far\n",
    "        maxer_abab = product_abab  # Update maxer_abab with the new lowest adjusted loss\n",
    "        epoch_adjusted_min = count_abab  # Record the epoch corresponding to the lowest adjusted loss\n",
    "\n",
    "        \n",
    "# Plotting all metrics and vertical lines at epochs of interest\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ensemble_val_profits1, label='ensemble models')\n",
    "plt.plot(model1, label='model1')\n",
    "plt.plot(model2, label='model2')\n",
    "plt.plot(model3, label='model3')\n",
    "plt.plot(model4, label='model4')\n",
    "plt.plot(model5, label='model5')\n",
    "plt.plot(model6, label='model6')\n",
    "plt.plot(model7, label='model7')\n",
    "plt.plot(model8, label='model8')\n",
    "plt.plot(model9, label='model9')\n",
    "plt.plot(model10, label='model10')\n",
    "plt.plot(model11, label='model11')\n",
    "\n",
    "plt.plot(training_losses, label='Training Loss', linestyle='--')\n",
    "plt.plot(validation_losses, label='Validation Loss', linestyle='--')\n",
    "plt.plot(combined_losses, label='Combined Loss', linestyle='--')\n",
    "\n",
    "\n",
    "plt.title('Validation Profits and Losses Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Profit')\n",
    "plt.legend()\n",
    "plt.show() # add running average of val profit1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a4686-0ff0-4eca-9f41-f15b1d9febee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all metrics and vertical lines at epochs of interest\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ensemble_val_profits1, label='ensemble models')\n",
    "plt.plot(model1, label='model1')\n",
    "plt.plot(model2, label='model2')\n",
    "plt.plot(model3, label='model3')\n",
    "plt.plot(model4, label='model4')\n",
    "plt.plot(model5, label='model5')\n",
    "plt.plot(model6, label='model6')\n",
    "plt.plot(model7, label='model7')\n",
    "plt.plot(model8, label='model8')\n",
    "plt.plot(model9, label='model9')\n",
    "plt.plot(model10, label='model10')\n",
    "plt.plot(model11, label='model11')\n",
    "\n",
    "plt.plot(training_losses, label='Training Loss', linestyle='--')\n",
    "plt.plot(validation_losses, label='Validation Loss', linestyle='--')\n",
    "plt.plot(combined_losses, label='Combined Loss', linestyle='--')\n",
    "\n",
    "\n",
    "plt.title('Validation Profits and Losses Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Profit')\n",
    "plt.legend()\n",
    "plt.show() # add running average of val profit1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6747b6-06cc-4390-b462-b117bef812c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ensemble_val_profits1, label='ensemble models')\n",
    "plt.plot(model1, label='model1')\n",
    "plt.plot(model2, label='model2')\n",
    "plt.plot(model3, label='model3')\n",
    "\n",
    "plt.title('Validation Profits and Losses Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Profit')\n",
    "plt.show() # add running average of val profit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedc129-253e-4ec5-b5a5-c1a7676173b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5ea8f-557e-4cff-9adb-14028a3ad83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8862ec-19b8-42ce-ab77-4cb2d8d42721",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = []\n",
    "for entry in range(100,120):\n",
    "    for sl in range(150,170):\n",
    "        strategies.append((69, entry, sl))\n",
    "strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0b396-74d8-4c69-8258-f36a2071e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your test tensors ready as X_test_tensor and Y_test_tensor\n",
    "\n",
    "# Evaluate each model on the test set\n",
    "test_predictions = [model(X_test_tensor) for model in models]\n",
    "\n",
    "# Combine predictions - here we'll average them, adjust if your task is different\n",
    "ensemble_predictions = torch.stack(test_predictions).mean(0)\n",
    "\n",
    "# Assuming a regression task, calculate RMSE as an example metric\n",
    "# Adjust this block if you're working on a classification task\n",
    "criterion = torch.nn.MSELoss()\n",
    "test_loss = criterion(ensemble_predictions, Y_test_tensor)\n",
    "test_rmse = torch.sqrt(test_loss)\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse.item()}\")\n",
    "\n",
    "# If it's a classification task and you need accuracy, you might do something like:\n",
    "# _, predicted_labels = torch.max(ensemble_predictions, 1)\n",
    "# accuracy = (predicted_labels == Y_test_tensor).float().mean()\n",
    "# print(f\"Test Accuracy: {accuracy.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc2fd1-cb86-41f7-8508-8a21d17cca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Retrain if the model's performance is below the threshold\n",
    "    if val_profit < retrain_threshold:\n",
    "        print(f\"Retraining model {i+1} due to low performance.\")\n",
    "        \n",
    "        # Reset model, optimizer, and scheduler for retraining\n",
    "        model = TransformerModel(num_features=13, num_classes=len(strategies), dim_model=256, num_heads=8, num_layers=2, dropout=0.2)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.99999)\n",
    "        \n",
    "        for epoch in range(epochs):  # Retraining loop\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_train_tensor)\n",
    "            loss_train = criterion(outputs_train, Y_train_tensor)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Optional: Insert logging or validation checks here\n",
    "            \n",
    "    # Add the (re)trained model to the ensemble\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ae70b-a67c-4078-8ee6-f811bd113108",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_val_profits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57633d44-9848-471f-98a1-b0098c02b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a38251-2426-4377-b545-0af3c32dd72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example val_profits1 tensor\n",
    "tval_profits1 = torch.tensor(val_profits1)\n",
    "\n",
    "# Ensure val_profits1 has a length that is a multiple of 10\n",
    "length = len(val_profits1)\n",
    "if length % 10 != 0:\n",
    "    trim_length = length - (length % 10)  # Find the length up to the nearest multiple of 10\n",
    "    tval_profits1 = tval_profits1[:trim_length]  # Trim the tensor\n",
    "\n",
    "# Reshape to have each row contain 10 elements\n",
    "val_profits1_reshaped = tval_profits1.view(-1, 10)\n",
    "\n",
    "# Calculate the mean of each row (i.e., the average of every 10 elements)\n",
    "averages = val_profits1_reshaped.mean(dim=1)\n",
    "\n",
    "# Plot the averages\n",
    "plt.plot(averages.numpy(), label='Top 1 Profit Average per 10 Epochs')  # Convert to numpy array for matplotlib\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6b0a1-539c-4395-9578-bb6676e112ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOST IMPORTANT!!!\n",
    "\n",
    "\n",
    "# the current logits highets value - trade selected\n",
    "# the tp, entry, sl\n",
    "# the profit on that trade\n",
    "# the total profit\n",
    "\n",
    "# prints:\n",
    "# the current logits highets value - trade selected\n",
    "# the tp, entry, sl\n",
    "# the profit on that trade\n",
    "# the total profit\n",
    "\n",
    "file_path = '/Users/connargibbon/Desktop/numbers/inputs.xlsx'  # Update the file extension if needed\n",
    "data = pd.read_excel(file_path, header=None)\n",
    "X_train = data.iloc[:1100, :13].values\n",
    "M_train = data.iloc[:1100, 13].values\n",
    "N_train = data.iloc[:1100, 14].values\n",
    "\n",
    "X_val = data.iloc[1100:1267, :13].values\n",
    "M_val = data.iloc[1100:1267, 13].values\n",
    "N_val = data.iloc[1100:1267, 14].values\n",
    "# collect 40 more trades\n",
    "# comment out the training of the model\n",
    "# replace X_val, M_val, N_val with the new 40 trades and see how the model does\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Creating a list of strategies\n",
    "strategies = [(18, 50, 186), (69, 101, 186), (33, 78, 115), (33,79,115), (33,79,124),\n",
    "             (37,79,115), (37,79,124), (33,82,115), (37,82,115), (33,83,115),\n",
    "             (33,83,124), (37,83,115), (37,83,124), (-5,84,115), (33,84,115), \n",
    "             (37,84,115), (-5,85,115), (33,85,115), (37,85,115), (-5,86,115),\n",
    "             (33,86,115), (33,86,124), (37,86,115), (42,86,115), (42,86,124),\n",
    "             (-5,87,115), (-5,87,124), (18,87,115), (33,87,115), (37,87,115),\n",
    "             (42,87,115), (42,87,124), (-5,88,115), (-5,88,124), (33,88,115),\n",
    "             (37,88,115), (37,88,124), (42,88,115), (-5,89,115), (-5,89,124),\n",
    "             (33,89,115), (37,89,115), (37,89,124), (42,89,115), (-5,90,115), \n",
    "             (-5,90,124), (-5,91,115), (-5,92,115), (-5,93,115), (-5,94,115), \n",
    "             (-5,95,115), (-5,95,124), (-5,96,124), (-5,97,124), (53,97,124),\n",
    "             (-5,98,124), (33,98,124), (53,98,124), (-5,99,124), (33,99,124),\n",
    "             (42,99,124), (47,99,124), (53,99,124), (-5,100,124), (-5,100,133),\n",
    "             (18,100,124), (33,100,124), (42,100,124), (47,100,124), (53,100,124),\n",
    "             (-5,101,124), (-5,101,133), (18,101,124), (33,101,124), (37,101,124),\n",
    "             (42,101,124), (47,101,124), (53,101,124), (57,101,124), (-5,102,124),\n",
    "             (33,102,124), (42,102,124), (47,102,124), (53,102,124), (-5,103,124),\n",
    "             (-5,103,133), (18,103,124), (33,103,124), (33,103,133), (37,103,124),\n",
    "             (42,103,124), (47,103,124), (47,103,133), (53,103,124), (53,103,133),\n",
    "             (53,103,141), (57,103,124), (63,103,124), (69,103,154), (-5,104,124),\n",
    "             (-5,104,133), (18,104,124), (33,104,124), (37,104,124), (42,104,124),\n",
    "             (47,104,124), (47,104,133), (53,104,124), (53,104,133), (53,104,141),\n",
    "             (57,104,124), (63,104,124), (63,104,141), (69,104,154), (-5,105,133),\n",
    "             (47,105,133), (53,105,133), (53,105,141), (57,105,141), (63,105,141),\n",
    "             (69,105,154), (-5,106,133), (33,106,133), (47,106,133), (53,106,133),\n",
    "             (53,106,141), (57,106,141), (63,106,133), (63,106,141), (63,106,147),\n",
    "             (63,106,154), (69,106,141), (69,106,154), (69,106,160), (-5,107,133),\n",
    "             (53,107,133), (53,107,141), (63,107,133), (63,107,141), (63,107,154),\n",
    "             (69,107,141), (69,107,154), (69,107,160), (63,108,141), (69,108,154),\n",
    "             (-5,109,133), (47,109,133), (47,109,141), (53,109,133), (53,109,141),\n",
    "             (57,109,141), (63,109,141), (63,109,154), (69,109,141), (69,109,154),\n",
    "             (69,109,160), (47,110,133), (47,110,141), (53,110,133), (53,110,141),\n",
    "             (57,110,141), (63,110,141), (63,110,154), (69,110,141), (69,110,147),\n",
    "             (69,110,154), (69,110,160), (53,111,141), (63,111,141), (69,111,141),\n",
    "             (69,111,147), (69,111,154), (69,111,160), (69,112,154), (69,128,154),\n",
    "             (69,129,154), (42,130,154), (57,130,154), (63,130,154), (69,130,154),\n",
    "             (69,130,160), (69,130,186), (72,130,154), (76,130,154), (69,131,154),\n",
    "             (57,132,154), (63,132,154), (69,132,154), (69,132,160), (72,132,154),\n",
    "             (69,134,154), (72,134,154), (69,138,160), (69,139,160), (72,144,186),\n",
    "             (69,145,167), (69,145,175), (69,145,186), (72,145,186), (33,148,186)] \n",
    "\n",
    "# Prepare the targets\n",
    "Y_train = [[0 for _ in range(len(strategies))] for _ in range(1100)]\n",
    "Y_val = [[0 for _ in range(len(strategies))] for _ in range(len(X_val))]\n",
    "\n",
    "\n",
    "# Populate Y_train and Y_val based on your logic\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(strategies)):\n",
    "        if M_train[i] > strategies[j][1] and M_train[i] < strategies[j][2] and N_train[i] > (100 - strategies[j][0]):\n",
    "            number = ((strategies[j][1]- strategies[j][0])/(strategies[j][2]-strategies[j][1]))\n",
    "            Y_train[i][j] = number \n",
    "        elif M_train[i] > strategies[j][1] and M_train[i] > strategies[j][2]:\n",
    "            Y_train[i][j] = -1\n",
    "        elif M_train[i] > strategies[j][1] and M_train[i] < strategies[j][2] and N_train[i] < (100 - strategies[j][0]):\n",
    "            Y_train[i][j] = -1\n",
    "        elif M_train[i] < strategies[j][1]:\n",
    "            Y_train[i][j] = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    for j in range(len(strategies)):\n",
    "        if M_val[i] > strategies[j][1] and M_val[i] < strategies[j][2] and N_val[i] > (100 - strategies[j][0]):\n",
    "            number = ((strategies[j][1]- strategies[j][0])/(strategies[j][2]-strategies[j][1]))\n",
    "            Y_val[i][j] = number \n",
    "        elif M_val[i] > strategies[j][1] and M_val[i] > strategies[j][2]:\n",
    "            Y_val[i][j] = -1\n",
    "        elif M_val[i] > strategies[j][1] and M_val[i] < strategies[j][2] and N_val[i] < (100 - strategies[j][0]):\n",
    "            Y_val[i][j] = -1\n",
    "        elif M_val[i] < strategies[j][1]:\n",
    "            Y_val[i][j] = 0\n",
    "\n",
    "            \n",
    "logits = torch.tensor(Y_val)\n",
    "logits.shape\n",
    "count = 0\n",
    "for i in range(167):\n",
    "    count += logits[i][predicted[i]].item()  # Assuming logits is a tensor; convert to Python number with .item()\n",
    "    print(f'{i} {predicted[i].item()},  {strategies[predicted[i].item()]} profit: {logits[i][predicted[i].item()]} total profit: {count})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06dcdb-95ca-4de5-aba8-65d12fe0d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_profits1[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d6226-849f-4a63-86c5-cee1fa667263",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in val_profits1[-10:]:\n",
    "    c += i\n",
    "print(c, (c/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a40f6-33ad-44eb-99dd-fdb62389cfec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. You will need to provide a summary write-up of each method's results and how it provides insights to your project questions. \n",
    "    \n",
    "    Factor analysis is used to find underlying variables that are able to explain common variance in a data set, and by using a linear combination of the factors we might be able to explain a lot of the common variance in the dataset. The first step was to use PCA to decide how many factors we want, and I found this by using a scree plot to see how many factors explained at least 90% of the variance. After that, I made two heatmaps that described the amount of communality, which is the variability that is explained by a linear combination of the factors we chose, and we can see that Factor 1 is able to explain body (0.88), leg4 (0.74), leg3 (1.0), and leg2 (0.63). Factor 3 is able to explain wick (0.83) well, and Factor 4 is able to describe leg1 (0.63) well. The problem is that none of the factors are able to describe any of the slopes, price, or EMA very well and we can see that in the Uniqueness graph which explains how much of the variability cannot be described by a combination of the linear factors. This might mean that we need to increase the number of factors we are using, since it is not able to describe a lot of the dataset. It is very good that leg4’s variance is explained pretty well using the factors since that is what we want to predict, as it is how much price pulls back.\n",
    "\n",
    "    Neural Network + PCA (Method):\n",
    "    Purpose- We will use Neural Network to model strong nonlinear relationships within our dataset. This can help us isolate and locate subtle trends we might not have noticed before, by making sure only the significant components are fed into our model. Using a neural network allows us to capture complex, nonlinear relationships in the dataset that linear methods may overlook. By integrating PCA beforehand, we can distill the dataset to its most influential components, which helps prevent overfitting and allows the neural network to focus on significant trends and patterns relevant to predicting leg4.\n",
    "    Application- We will apply PCA to reduce dimensionality within our dataset, then design a neural network that will find the non linear patterns already within our data set. Our neural network trained on reduced PCA data will be precise and accurate in predicting leg 4. We can test this using RSME and MSE to see how the results are. After reducing the dataset's dimensionality through PCA, we construct a neural network tailored to learn the complex, nonlinear patterns that influence leg4. With the reduced PCA components as inputs, the neural network can train more efficiently, improving model accuracy and generalization. Performance is evaluated through metrics such as Root Mean Squared Error (RMSE) and Mean Squared Error (MSE), which allow us to assess prediction accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
